---
title: "Projection pursuit classification random forest "
author: "N. da Silva, E. Lee & D. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
fig_caption: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Vignette Title}
  \usepackage[utf8]{inputenc}
---



## Abstract

The `PPforest` package (projection pursuit random forest) contains functions to run a projection pursuit random forest for classification problems. This method utilize combinations of variables in each the tree construction.  In a random forest each split is based on a single variable, chosen from a subset of predictors. In the `PPforest`, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The `PPforest` uses the `PPtree` algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for `PPforest`.

##Introduction

`PPforest` package implements a random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest. For each split a random sample of variables is selected and a linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.

The classification method presented here `PPforest` uses the `PPtree` algorithm implemented in R  was  which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occur on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for `PPforest`.
To improve the speed performance `PPforest` package, `PPtree` algorithm was translated to Rcpp. 
The `PPforest` package (short for projection pursuit forest) contains functions to run a projection pursuit random forest for classification problems. The package utilizes a number of R packages some of them included in "suggests" not to load them all at package start-up1.
Install PPforest from github using:

```{r,echo=FALSE, message=FALSE, warning=FALSE,eval=FALSE}
library(devtools)
install_github("natydasilva/PPforest")
library(PPforest)
```



###Motivation Example
Australian crab data set contains measurements on rock crabs of the genus Leptograpsus. There are 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BlueMale, BlueFemale, OrangeMale and OrangeFemale). The data were collected on site at Fremantle, Western Australia. For each specimen, five measurements were made, using vernier calipers.

1. FL the size of the frontal lobe length, in mm
2. RW rear width, in mm
3. CL length of mid line of the carapace, in mm
4. CW maximum width of carapace, in mm
5. BD depth of the body; for females, measured after displacement of the abdomen, in mm

To visualize this data set we use a scatterplot matrix from the package `GGally` 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(PPforest)
require(RColorBrewer)
require(GGally)
require(gridExtra)
require(PPtreeViz)
require(plotly)
```


```{r,fig.show='hold', fig.width = 6, fig.height = 6, echo=FALSE}

a <- GGally::ggpairs(PPforest::crab,
    columns = 2:6,
    ggplot2::aes(colour = Type,alpha=.3),
    lower = list(continuous = 'points'),
    axisLabels='none',
    upper=list(continuous='blank')
     , legend = NULL)

a
```
In this figure we can see a strong, positive and linear association between the different variables. Also look like the classes can be separated by linear combinations.



##Projection pursuit classification forest
 in `PPforest`, projection pursuit classification trees  are used as the individual model to be combined in the forest. The original algorithn is in `PPtreeViz` package,  we translate the original tree algorithm into `Rcpp` to imporove the speed performance in to run the forest.  

One important characteristic of PPtree is that treats the data always as a two-class system,  when the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split.
Let  $(X_i,y_i)$ the data set, $X_i$ is a  p-dimensional vector of explanatory variables and  $y_i\in {1,2,\ldots G}$ represents class information with $i=1,\ldots n$.

In the first step optimize a projection pursuit index to find an optimal one-dimension projection $\alpha^*$ for separating all classes in the current data. With the projected data redefine the problem in a two class problem by comparing means, and assign a new label $G1$ or $G2$ to each observation, a new variable $y_i^*$ is created.  The new groups $G1$ and $G2$ can contain more than one original classes. Next step is to find an optimal one-dimensional projection $\alpha$, using $(X_i,y_i^*)$ to separate the two class problem $G1$ and $G2$. The best separation of $G1$ and $G2$ is determine in this step and the decision rule is defined for the current node, if $\sum_{i=1}^p \alpha_i M1< c$ then assign $G1$ to the left node else assign $G2$ to the right node, where $M1$ is the mean of $G1$.
For each groups we can repeat all the previous steps until $G1$ and $G2$ have only one class from the original classes. Base on this process to grow the tree, the depth of PPtree is at most the number of classes because one class is assigned only to one final node.

Trees from `PPtree` algorithm are simple, they use the association between variables to find separation. If a linear boundary exists, `PPtree` produces a tree without misclassification.

Projection pursuit random forest algorithm description

1. Let N the number of cases in the training set $\Theta=(X,Y)$, $B$ bootstrap samples from the training set are taking (samples of size N with replacement)
2. For each bootstrap sample a `PPtree`  is grown to the largest extent possible $h(x, {\Theta_k})$. No pruning. This tree is grown using step 3 modification.
3. Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and the best split based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.
4.  Predict the classes of each case not included in the bootstrap sample and compute oob error.
5.  Based on majority vote predict the class for new data.

###Overview PPforest package
`PPforest` package implements a classification random forest using projection pursuit classification trees. The following table present all the functions in `PPforest` package.

| Function |Description |
| ----------------- | --------------------------------------------------------------  | 
|PPforest|Runs a Projection pursuit random forest|
|baggtree|For each bootstrap sample grow a projection persuit tree (PPtree object).|
|parallel_dat|Parallel plot for the data set colored by class, the data are standarized in the function|
|permute_importance2|Obtain the permuted importance variable measure|
|ppf_importance| Plot a global measure of importance in PPforest.|
|ppf_oob_error| OOB error visualization|
|predict.PPforest|Vector with predicted values from a PPforest object|
|pproxy_plot| Proximity matrix visualization|
|PPtree_split|Projection pursuit classification tree with random variable selection in each split|
|tree_ppred|Vector with predicted values from a PPtree object.|
|predict.PPforest|Predict class for the test set and calculate prediction error|
|print.PPforest| Print PPforest object|
|vote_viz1|Vote matrix visualization|

Also `PPforest` package includes some data set that were used to test the predictive performance of our method. The data sets included are: crab, fishcatch, glass, image, leukemia, lymphoma NCI60, parkinson and wine.

The main function of the package is `PPforest` which implements a projection pursuit random forest.
`parallel_dat`, `ppf_importance`, `ppf_oob_error`, `proxy_plot` and `vote_viz` are funcions to visuzlize some of the main diagnostics in `PPforest`. The main diagnostics are; vote matrix, proximity matrix, importance variable measure and error rat.

`PPtree_split` this function implements a projection pursuit classification tree with random variable selection in each split, based on the original PPtreeViz algorithm. This function returns a `PPtreeclass` object.
To use this function we need to specify a formula describing the model to be fitted response\~predictors (`form`),
`data` is a data frame with the complete data set. Also we need to specify the method `PPmethod`, it is the index to use for projection pursuit: 'LDA' or 'PDA', 
`size.p` is the proportion of variables randomly sampled in each split. If size.p = 1 a classic `PPtreeclass` object will be fitted using all the variables in each node partition instead of a subset of them.
`lambda` penalty parameter in PDA index and is between 0 to 1 .
he following example fits a projection pursuit classification tree constructed using 0.6 of the variables (3 out of 5) in each node split. We selected `LDA` method.
```{r}
Tree.crab <- PPforest::PPtree_split("Type~.", data = crab, PPmethod = "LDA", size.p = 0.6)
 Tree.crab
```

`baggtree` this function grow a `PPtreeclass` using `PPtree_split` for each bootstrap sample.
This function returns a data frame with the results from `PPtree_split` for each bootsrap samples.
`m` is th number of trees here is a small example.
```{r tidy=FALSE}
crab.trees <- baggtree(data = crab, class = "Type", 
 m =  10, PPmethod = 'LDA', size.p = 0.6 ) 
str(crab.trees, max.level = 1)

#selecting first PPtree object, 
crab.trees[[1]][[1]]
```